{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs created for female formal: 1435\n",
      "Number of pairs created for female informal: 16899797\n",
      "Number of pairs created for male formal: 179676\n",
      "Number of pairs created for male informal: 46413\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Base directory where original images are stored\n",
    "base_dir = 'C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/image_data'\n",
    "\n",
    "# Directory to save processed images\n",
    "processed_dir = 'C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/processed_image_data'\n",
    "\n",
    "# Create the processed image directory structure\n",
    "def create_dir_structure(base_dir, processed_dir):\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for dir_name in dirs:\n",
    "            # Create corresponding directory in the processed folder\n",
    "            dir_path = os.path.join(processed_dir, os.path.relpath(os.path.join(root, dir_name), base_dir))\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "create_dir_structure(base_dir, processed_dir)\n",
    "\n",
    "# Image preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to denormalize the image before saving (for visualization)\n",
    "def denormalize(tensor, mean, std):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Reverse the normalization\n",
    "    return tensor\n",
    "\n",
    "# Function to load, preprocess, and save images\n",
    "def load_preprocess_and_save_image(image_path, save_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    tensor = transform(image)\n",
    "    \n",
    "    # Denormalize before saving for correct visualization\n",
    "    denorm_tensor = denormalize(tensor.clone(), mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    # Convert back to PIL image for saving\n",
    "    save_image = transforms.ToPILImage()(denorm_tensor)\n",
    "    save_image.save(save_path)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# Function to load image paths from a given directory\n",
    "def load_image_paths(root_dir):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):  # Case-insensitive check\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "# Function to process images in a given category and save them in the corresponding processed directory\n",
    "def process_images_in_category(category_dir, save_category_dir):\n",
    "    image_paths = load_image_paths(category_dir)\n",
    "    processed_images = []\n",
    "    for image_path in image_paths:\n",
    "        # Determine where to save the processed image\n",
    "        relative_path = os.path.relpath(image_path, category_dir)\n",
    "        save_path = os.path.join(save_category_dir, relative_path)\n",
    "        \n",
    "        # Process and save the image\n",
    "        processed_images.append(load_preprocess_and_save_image(image_path, save_path))\n",
    "    \n",
    "    return processed_images\n",
    "\n",
    "# Function to create pairs within two categories\n",
    "def create_pairs(category1, category2):\n",
    "    pairs = list(itertools.product(category1, category2))\n",
    "    return pairs\n",
    "\n",
    "# Process images in each category and save them\n",
    "# Female\n",
    "female_formal_bottoms = process_images_in_category(os.path.join(base_dir, 'female/formal/bottoms'), os.path.join(processed_dir, 'female/formal/bottoms'))\n",
    "female_formal_tops = process_images_in_category(os.path.join(base_dir, 'female/formal/tops'), os.path.join(processed_dir, 'female/formal/tops'))\n",
    "\n",
    "female_informal_bottoms = process_images_in_category(os.path.join(base_dir, 'female/informal/bottoms'), os.path.join(processed_dir, 'female/informal/bottoms'))\n",
    "female_informal_tops = process_images_in_category(os.path.join(base_dir, 'female/informal/tops'), os.path.join(processed_dir, 'female/informal/tops'))\n",
    "\n",
    "# Male\n",
    "male_formal_bottoms = process_images_in_category(os.path.join(base_dir, 'male/formal/bottoms'), os.path.join(processed_dir, 'male/formal/bottoms'))\n",
    "male_formal_tops = process_images_in_category(os.path.join(base_dir, 'male/formal/tops'), os.path.join(processed_dir, 'male/formal/tops'))\n",
    "\n",
    "male_informal_bottoms = process_images_in_category(os.path.join(base_dir, 'male/informal/bottoms'), os.path.join(processed_dir, 'male/informal/bottoms'))\n",
    "male_informal_tops = process_images_in_category(os.path.join(base_dir, 'male/informal/tops'), os.path.join(processed_dir, 'male/informal/tops'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs created for female formal: 1435\n",
      "Number of pairs created for female informal: 16899797\n",
      "Number of pairs created for male formal: 179676\n",
      "Number of pairs created for male informal: 46413\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import itertools\n",
    "\n",
    "# Directory where processed images are stored\n",
    "processed_dir = 'C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/processed_image_data'\n",
    "\n",
    "# Image preprocessing transformations (same as used for processing)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to load images from a given directory\n",
    "def load_images_from_dir(dir_path):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):  # Ensure only image files are loaded\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        tensor = transform(image)\n",
    "        images.append(tensor)\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Load the images from the processed directories\n",
    "female_formal_tops = load_images_from_dir(os.path.join(processed_dir, 'female/formal/tops'))\n",
    "female_formal_bottoms = load_images_from_dir(os.path.join(processed_dir, 'female/formal/bottoms'))\n",
    "\n",
    "female_informal_tops = load_images_from_dir(os.path.join(processed_dir, 'female/informal/tops'))\n",
    "female_informal_bottoms = load_images_from_dir(os.path.join(processed_dir, 'female/informal/bottoms'))\n",
    "\n",
    "male_formal_tops = load_images_from_dir(os.path.join(processed_dir, 'male/formal/tops'))\n",
    "male_formal_bottoms = load_images_from_dir(os.path.join(processed_dir, 'male/formal/bottoms'))\n",
    "\n",
    "male_informal_tops = load_images_from_dir(os.path.join(processed_dir, 'male/informal/tops'))\n",
    "male_informal_bottoms = load_images_from_dir(os.path.join(processed_dir, 'male/informal/bottoms'))\n",
    "\n",
    "# Function to create pairs within two categories\n",
    "def create_pairs(category1, category2):\n",
    "    pairs = list(itertools.product(category1, category2))\n",
    "    return pairs\n",
    "\n",
    "# Recreate the pairs\n",
    "female_formal_pairs = create_pairs(female_formal_tops, female_formal_bottoms)\n",
    "female_informal_pairs = create_pairs(female_informal_tops, female_informal_bottoms)\n",
    "\n",
    "male_formal_pairs = create_pairs(male_formal_tops, male_formal_bottoms)\n",
    "male_informal_pairs = create_pairs(male_informal_tops, male_informal_bottoms)\n",
    "\n",
    "# Print the number of pairs created\n",
    "print(f\"Number of pairs created for female formal: {len(female_formal_pairs)}\")\n",
    "print(f\"Number of pairs created for female informal: {len(female_informal_pairs)}\")\n",
    "print(f\"Number of pairs created for male formal: {len(male_formal_pairs)}\")\n",
    "print(f\"Number of pairs created for male informal: {len(male_informal_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs after downsampling for female informal: 50000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# If there are too many pairs, you can randomly select a subset\n",
    "# For example, let's say we want only 50,000 pairs out of the 16M+ pairs for female informal\n",
    "desired_num_pairs = 50000\n",
    "if len(female_informal_pairs) > desired_num_pairs:\n",
    "    female_informal_pairs = random.sample(female_informal_pairs, desired_num_pairs)\n",
    "\n",
    "print(f\"Number of pairs after downsampling for female informal: {len(female_informal_pairs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of female formal labels: 1435\n",
      "Number of female informal labels: 50000\n",
      "Number of male formal labels: 179676\n",
      "Number of male informal labels: 46413\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Example heuristic for synthetic labels\n",
    "def generate_synthetic_labels(pairs):\n",
    "    labels = []\n",
    "    for (img1, img2) in pairs:\n",
    "        # Simple heuristic: label pairs from the same category as 1 (compatible)\n",
    "        # and from different categories as 0 (incompatible)\n",
    "        if img1.shape == img2.shape:  # You might replace this condition with a more relevant feature\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels\n",
    "\n",
    "# Generate synthetic labels for each of your pairs\n",
    "female_formal_labels = generate_synthetic_labels(female_formal_pairs)\n",
    "female_informal_labels = generate_synthetic_labels(female_informal_pairs)\n",
    "male_formal_labels = generate_synthetic_labels(male_formal_pairs)\n",
    "male_informal_labels = generate_synthetic_labels(male_informal_pairs)\n",
    "\n",
    "print(f\"Number of female formal labels: {len(female_formal_labels)}\")\n",
    "print(f\"Number of female informal labels: {len(female_informal_labels)}\")\n",
    "print(f\"Number of male formal labels: {len(male_formal_labels)}\")\n",
    "print(f\"Number of male informal labels: {len(male_informal_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of female formal pairs: 1435\n",
      "Number of female formal labels: 1435\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def create_tensors_from_pairs_in_batches(pairs, labels, batch_size):\n",
    "    num_batches = len(pairs) // batch_size + (1 if len(pairs) % batch_size != 0 else 0)\n",
    "    print(f\"Number of batches: {num_batches}\")  # Debug: Check the number of batches\n",
    "    for i in range(num_batches):\n",
    "        batch_pairs = pairs[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_labels = labels[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "        print(f\"Processing batch {i+1}/{num_batches}\")  # Debug: Indicate which batch is being processed\n",
    "        \n",
    "        pairs_tensor = torch.stack([pair_to_tensor(pair) for pair in batch_pairs])\n",
    "        labels_tensor = torch.tensor(batch_labels, dtype=torch.float32)\n",
    "        \n",
    "        yield pairs_tensor, labels_tensor\n",
    "\n",
    "# Check the number of pairs and labels\n",
    "print(f\"Number of female formal pairs: {len(female_formal_pairs)}\")\n",
    "print(f\"Number of female formal labels: {len(female_formal_labels)}\")\n",
    "\n",
    "# Set your batch size here\n",
    "batch_size = 100  # Adjust this depending on your system's memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the Compatibility Model\n",
    "class CompatibilityModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompatibilityModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * 3 * 224 * 224, 512),  # Adjust input size based on image dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # Output between 0 and 1 for compatibility score\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = CompatibilityModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/10\n",
      "Number of batches: 15\n",
      "Processing batch 1/15\n",
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Epoch [1/10], Loss: 0.7481\n",
      "Starting epoch 2/10\n",
      "Number of batches: 15\n",
      "Processing batch 1/15\n",
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Epoch [2/10], Loss: 0.7481\n",
      "Starting epoch 3/10\n",
      "Number of batches: 15\n",
      "Processing batch 1/15\n",
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Epoch [3/10], Loss: 0.7481\n",
      "Starting epoch 4/10\n",
      "Number of batches: 15\n",
      "Processing batch 1/15\n",
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Epoch [4/10], Loss: 0.7481\n",
      "Starting epoch 5/10\n",
      "Number of batches: 15\n",
      "Processing batch 1/15\n",
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Epoch [5/10], Loss: 0.7481\n",
      "Starting epoch 6/10\n",
      "Number of batches: 15\n",
      "Processing batch 1/15\n",
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Epoch [6/10], Loss: 0.7481\n",
      "Starting epoch 7/10\n",
      "Number of batches: 15\n",
      "Processing batch 1/15\n",
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Epoch [7/10], Loss: 0.7481\n",
      "Starting epoch 8/10\n",
      "Number of batches: 15\n",
      "Processing batch 1/15\n",
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Epoch [8/10], Loss: 0.7481\n",
      "Starting epoch 9/10\n",
      "Number of batches: 15\n",
      "Processing batch 1/15\n",
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Epoch [9/10], Loss: 0.7481\n",
      "Starting epoch 10/10\n",
      "Number of batches: 15\n",
      "Processing batch 1/15\n",
      "Processing batch 2/15\n",
      "Processing batch 3/15\n",
      "Processing batch 4/15\n",
      "Processing batch 5/15\n",
      "Processing batch 6/15\n",
      "Processing batch 7/15\n",
      "Processing batch 8/15\n",
      "Processing batch 9/15\n",
      "Processing batch 10/15\n",
      "Processing batch 11/15\n",
      "Processing batch 12/15\n",
      "Processing batch 13/15\n",
      "Processing batch 14/15\n",
      "Processing batch 15/15\n",
      "Epoch [10/10], Loss: 0.7481\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 100  # Adjust based on your memory capacity\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting epoch {epoch+1}/{num_epochs}\")  # Debug: Indicate the start of an epoch\n",
    "    for batch_pairs, batch_labels in create_tensors_from_pairs_in_batches(female_formal_pairs, female_formal_labels, batch_size):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_pairs)\n",
    "        loss = criterion(outputs.squeeze(), batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Define the directory where you want to save the model\n",
    "save_directory = 'C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/backend/ml_models'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Define the full path for saving the model\n",
    "model_filename = 'compatibility_model.pth'\n",
    "model_path = os.path.join(save_directory, model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/backend/ml_models\\compatibility_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your trained model instance\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ARA\\AppData\\Local\\Temp\\ipykernel_28548\\205520513.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompatibilityModel(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=301056, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = CompatibilityModel()\n",
    "\n",
    "# Load the saved model weights\n",
    "model_path = 'C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/backend/ml_models/compatibility_model.pth'\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ARA\\AppData\\Local\\Temp\\ipykernel_28548\\4133237021.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation 1: C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/processed_image_data\\female/informal/bottoms\\WOMEN-Pants-id_00004253-07_1_front.png with score 0.5225\n",
      "Recommendation 2: C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/processed_image_data\\female/informal/bottoms\\WOMEN-Pants-id_00007168-04_1_front.png with score 0.5216\n",
      "Recommendation 3: C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/processed_image_data\\female/informal/bottoms\\WOMEN-Skirts-id_00001503-03_1_front.png with score 0.5215\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the directory where preprocessed images are stored\n",
    "preprocessed_dir = 'C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/processed_image_data'\n",
    "\n",
    "# Define the model architecture and load the trained model\n",
    "model = CompatibilityModel()\n",
    "model_path = 'C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/backend/ml_models/compatibility_model.pth'\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Function to load a preprocessed image\n",
    "def load_preprocessed_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.ToTensor()  # No need to resize or normalize since it's already preprocessed\n",
    "    tensor = transform(image)\n",
    "    return tensor\n",
    "\n",
    "# Function to create a pair tensor\n",
    "def create_pair_tensor(image1_tensor, image2_tensor):\n",
    "    return torch.cat((image1_tensor.unsqueeze(0), image2_tensor.unsqueeze(0)), dim=1)\n",
    "\n",
    "# Function to get the top 3 bottom recommendations for a selected top\n",
    "def get_top_bottom_recommendations(selected_top_path, bottoms_dir, model, num_recommendations=3):\n",
    "    # Load the selected top\n",
    "    selected_top_tensor = load_preprocessed_image(selected_top_path)\n",
    "\n",
    "    # Load all bottoms in the specified directory\n",
    "    bottom_images = [os.path.join(bottoms_dir, file) for file in os.listdir(bottoms_dir) if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    scores = []\n",
    "    for bottom_path in bottom_images:\n",
    "        bottom_tensor = load_preprocessed_image(bottom_path)\n",
    "        pair_tensor = create_pair_tensor(selected_top_tensor, bottom_tensor)\n",
    "        with torch.no_grad():\n",
    "            score = model(pair_tensor.unsqueeze(0)).item()\n",
    "        scores.append((score, bottom_path))\n",
    "\n",
    "    # Sort scores in descending order and select top 3\n",
    "    scores.sort(reverse=True, key=lambda x: x[0])\n",
    "    top_recommendations = scores[:num_recommendations]\n",
    "\n",
    "    return top_recommendations\n",
    "\n",
    "# Example of usage\n",
    "selected_top_path = os.path.join(preprocessed_dir, 'female/informal/tops/00009_00.png')\n",
    "bottoms_dir = os.path.join(preprocessed_dir, 'female/informal/bottoms')\n",
    "\n",
    "top_3_recommendations = get_top_bottom_recommendations(selected_top_path, bottoms_dir, model)\n",
    "\n",
    "# Print the top 3 recommendations\n",
    "for i, (score, bottom_path) in enumerate(top_3_recommendations, start=1):\n",
    "    print(f\"Recommendation {i}: {bottom_path} with score {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the selected top image and preprocess it\n",
    "def load_preprocessed_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.ToTensor()\n",
    "    tensor = transform(image)\n",
    "    return tensor\n",
    "\n",
    "selected_top_path = 'C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/processed_image_data/female/informal/tops/00009_00.png'\n",
    "selected_top_tensor = load_preprocessed_image(selected_top_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ARA\\AppData\\Local\\Temp\\ipykernel_28548\\3100298305.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inference time: 106.1569 seconds\n",
      "Recommendation 1: C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/processed_image_data/female/informal/bottoms\\WOMEN-Pants-id_00004253-07_1_front.png with score 0.5225\n",
      "Recommendation 2: C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/processed_image_data/female/informal/bottoms\\WOMEN-Pants-id_00007168-04_1_front.png with score 0.5216\n",
      "Recommendation 3: C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/processed_image_data/female/informal/bottoms\\WOMEN-Skirts-id_00001503-03_1_front.png with score 0.5215\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the bottoms to compare against\n",
    "bottoms_dir = 'C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/media/processed_image_data/female/informal/bottoms'\n",
    "model = CompatibilityModel()\n",
    "model_path = 'C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/backend/ml_models/compatibility_model.pth'\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Compare the selected top with all available bottoms\n",
    "def get_live_recommendations(selected_top_tensor, bottoms_dir, model, num_recommendations=3):\n",
    "    bottom_images = [os.path.join(bottoms_dir, file) for file in os.listdir(bottoms_dir) if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    scores = []\n",
    "    start_time = time.time()  # Start timing\n",
    "    for bottom_path in bottom_images:\n",
    "        bottom_tensor = load_preprocessed_image(bottom_path)\n",
    "        pair_tensor = torch.cat((selected_top_tensor.unsqueeze(0), bottom_tensor.unsqueeze(0)), dim=1)\n",
    "        with torch.no_grad():\n",
    "            score = model(pair_tensor.unsqueeze(0)).item()\n",
    "        scores.append((score, bottom_path))\n",
    "    \n",
    "    total_time = time.time() - start_time  # End timing\n",
    "    \n",
    "    # Sort scores in descending order and select top 3\n",
    "    scores.sort(reverse=True, key=lambda x: x[0])\n",
    "    top_recommendations = scores[:num_recommendations]\n",
    "    \n",
    "    return top_recommendations, total_time\n",
    "\n",
    "# Example usage\n",
    "top_3_recommendations, inference_time = get_live_recommendations(selected_top_tensor, bottoms_dir, model)\n",
    "print(f\"Total inference time: {inference_time:.4f} seconds\")\n",
    "\n",
    "# Display the recommendations\n",
    "for i, (score, bottom_path) in enumerate(top_3_recommendations, start=1):\n",
    "    print(f\"Recommendation {i}: {bottom_path} with score {score:.4f}\")\n",
    "    bottom_image = Image.open(bottom_path)\n",
    "    bottom_image.show(title=f\"Recommendation {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the Compatibility Model\n",
    "class CompatibilityModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CompatibilityModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * 3 * 224 * 224, 512),  # Adjust input size based on image dimensions\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()  # Output between 0 and 1 for compatibility score\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = CompatibilityModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ARA\\AppData\\Local\\Temp\\ipykernel_44392\\205520513.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompatibilityModel(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=301056, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = CompatibilityModel()\n",
    "\n",
    "# Load the saved model weights\n",
    "model_path = 'C:/Users/ARA/Desktop/OUTFIT-AURA-web-application-/backend/ml_models/compatibility_model.pth'\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ARA\\AppData\\Local\\Temp\\ipykernel_44392\\866587769.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation 1: C:/Users/ARA/Desktop/ara330/media/processed_image_data\\female/informal/tops\\01997_00.png with score 0.5197\n",
      "Recommendation 2: C:/Users/ARA/Desktop/ara330/media/processed_image_data\\female/informal/tops\\13832_00.png with score 0.5182\n",
      "Recommendation 3: C:/Users/ARA/Desktop/ara330/media/processed_image_data\\female/informal/tops\\10226_00.png with score 0.5178\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import json\n",
    "\n",
    "# Define the directory where preprocessed images are stored\n",
    "preprocessed_dir = 'C:/Users/ARA/Desktop/ara330/media/processed_image_data'\n",
    "\n",
    "# Define the model architecture and load the trained model\n",
    "model = CompatibilityModel()\n",
    "model_path = 'C:/Users/ARA/Desktop/ara330/backend/ml_models/compatibility_model.pth'\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Function to load a preprocessed image\n",
    "def load_preprocessed_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.ToTensor()  # No need to resize or normalize since it's already preprocessed\n",
    "    tensor = transform(image)\n",
    "    return tensor\n",
    "\n",
    "# Function to create a pair tensor\n",
    "def create_pair_tensor(image1_tensor, image2_tensor):\n",
    "    return torch.cat((image1_tensor.unsqueeze(0), image2_tensor.unsqueeze(0)), dim=1)\n",
    "\n",
    "# Function to save the scores to a JSON file\n",
    "def save_scores(scores, save_path):\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(scores, f)\n",
    "\n",
    "# Function to load the scores from a JSON file\n",
    "def load_scores(save_path):\n",
    "    with open(save_path, 'r') as f:\n",
    "        scores = json.load(f)\n",
    "    return scores\n",
    "\n",
    "# Function to get the top 3 bottom recommendations for a selected top\n",
    "def get_top_bottom_recommendations(selected_top_path, bottoms_dir, model, num_recommendations=3, save_path=None):\n",
    "    if save_path and os.path.exists(save_path):\n",
    "        # Load saved scores if available\n",
    "        scores = load_scores(save_path)\n",
    "    else:\n",
    "        # Calculate scores if not already saved\n",
    "        selected_top_tensor = load_preprocessed_image(selected_top_path)\n",
    "        bottom_images = [os.path.join(bottoms_dir, file) for file in os.listdir(bottoms_dir) if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "        scores = []\n",
    "        for bottom_path in bottom_images:\n",
    "            bottom_tensor = load_preprocessed_image(bottom_path)\n",
    "            pair_tensor = create_pair_tensor(selected_top_tensor, bottom_tensor)\n",
    "            with torch.no_grad():\n",
    "                score = model(pair_tensor.unsqueeze(0)).item()\n",
    "            scores.append((score, bottom_path))\n",
    "\n",
    "        # Save scores for future use\n",
    "        if save_path:\n",
    "            save_scores(scores, save_path)\n",
    "\n",
    "    # Sort scores in descending order and select top 3\n",
    "    scores.sort(reverse=True, key=lambda x: x[0])\n",
    "    top_recommendations = scores[:num_recommendations]\n",
    "\n",
    "    return top_recommendations\n",
    "\n",
    "# Example of usage\n",
    "selected_top_path = os.path.join(preprocessed_dir, 'female/informal/bottoms/WOMEN-Pants-id_00001917-01_1_front.png')\n",
    "bottoms_dir = os.path.join(preprocessed_dir, 'female/informal/tops')\n",
    "save_path = os.path.join(preprocessed_dir, 'female/informal/pairs_scores.json')\n",
    "\n",
    "top_3_recommendations = get_top_bottom_recommendations(selected_top_path, bottoms_dir, model, save_path=save_path)\n",
    "\n",
    "# Print the top 3 recommendations\n",
    "for i, (score, bottom_path) in enumerate(top_3_recommendations, start=1):\n",
    "    print(f\"Recommendation {i}: {bottom_path} with score {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outfit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
